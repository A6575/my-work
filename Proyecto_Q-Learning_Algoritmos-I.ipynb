{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad Simón Bolívar\n",
        "## CI-2691 Laboratorio de Algoritmos y Estructuras I\n",
        "## Estudiante: Astrid Alvarado. Carnet: 18-10938\n",
        "## Proyecto"
      ],
      "metadata": {
        "id": "3Fv02c4WoOZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendizaje por Refuerzo\n",
        "\n",
        "Se conoce como *Aprendizaje por Refuerzo* al aprendizaje en el que un agente, con el tiempo, se instruye para comportarse de manera ópitma en un entorno determinado. Este aprendizaje se consigue mediante la interacción continua con dicho entorno en donde experimenta varias situaciones, denominadas estados, en los cuales puede elegir entre un conjunto de acciones permitidas, obteniendo así recompensas o sanciones. Es así como el agente consigue aprender a maximizar estas recompensas con el fin de comportarse de manera impecable en cualquier estado dado en el que se encuentre.\n",
        "\n",
        "Uno de las presentaciones más básicas de este aprendizaje es ***Q-Learning***, con el cual se busca mejorar iterativamente el comportamiento del agente con respecto a su aprendizaje mediante el uso de valores Q.\n",
        "\n",
        "Para su aplicación, se emplea lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "*   Valores de acción o valores Q (**Q(S,A)**): son la estimación de tomar cierta acción A en el estado actual S. Estos se calculan de forma iterativa usando la Ecuación de Bellman.\n",
        "*   Recompensas y episodios: el agente realiza una serie de transiciones en función a su elección de acción y también del entorno en el que interactúa. Durante la transición, el agente toma una acción en el estado actual, observa la recompensa del entorno y luego pasa a otro estado. Si el agente llega a alguno de los estados terminales (zona objetivo o zonas por las cuales no puede pasar), querrá decir que no hay más transición posible, por lo tanto, representa el final de un episodio.\n",
        "*   Ecuación de Bellman: se representa de la siguiente manera:\n",
        "\n",
        "    Q(S, A) ← Q(S, A) + α[R + γQ(S', A') - Q(S, A)]\n",
        "    \n",
        "    donde:\n",
        "      *   S: Estado actual del agente.\n",
        "      *   A: Acción actual seleccionada de acuerdo con la política *ϵ-greedy*.\n",
        "      * S': Siguiente estado donde va a parar el agente.\n",
        "      * A': La siguiente mejor acción que se seleccionará utilizando el valor Q máximo en el siguiente estado.\n",
        "      * R: Recompensa actual observada en el ambiente en respuesta a la acción actual.\n",
        "      * γ (0< γ <= 1): Factor de descuento para recompensas futuras.\n",
        "      * α (0< α <= 1): Ratio de aprendizaje que se toma para actualizar la estimación Q(S, A).\n",
        "\n",
        "*   Elección de la acción a ejecutar con la política de *ϵ-greedy*: el agente interactúa con el ambiente de una de dos formas. La primera, denominada **explotación**, se basa en escoger la siguiente acción tomando como referencia el máximo valor de los valores Q; la segunda, denominada **exploración**, se basa en la selección de una acción a realizar de manera aleatoria. La política de *ϵ-greedy* sirve para balancear esta selección.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zEDhBnLbgIw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto: uso de Q-Learning para la exploración de Marte\n",
        "\n",
        "A manera de ilustrar, se realizará un programa inspirado en la misión Mars 2020, se busca emular el aprendizaje del helicóptero Ingenuity para poder volar y explorar el planeta Marte. El objetivo del robot es conseguir el camino más corto desde donde se encuentre hasta la posición objetivo."
      ],
      "metadata": {
        "id": "6jJt-ks-fCqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación"
      ],
      "metadata": {
        "id": "LQqX5soUgInk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correr celda para importar las librerías necesarias para\n",
        "# el correcto funcionamiento de las siguiente funciones a implementar\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import colors\n",
        "from time import sleep"
      ],
      "metadata": {
        "id": "wtrlfSSPg7zC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entorno\n",
        "\n",
        "Para crear el entorno donde el agente interactuará, se dibujará un mapa mediante el uso de una lista NxN cuyos elementos se encuentran en el intervalo [0,4]. El terreno estará compuesto por rocas altamente filosas que destruyen por completo al robot si llegase a chocar con estas(rojo oscuro, representado por el número 1), y si el usuario desea más complejidad, añadir cuantas áreas quiera de arenas nocivas que pueden dañar el mecanismo del robot y que este sólo decida cruzar en caso de ser necesario (marrón claro, representado por el número 3). Por lo tanto se tiene que las zonas seguras para explorar Marte serán sobrevolar el terreno por el cielo (morado claro, representado por el número 0) y terreno liso (rojo, representado por el número 2), siendo el cielo la zona más segura para el agente.\n",
        "\n",
        "Una vez seleccionado el punto objetivo para el robot, será marcado en el mapa (verde, representado por el número 4)\n",
        "\n",
        "Por lo tanto, el agente deberá ser capaz de aprender a llegar al punto objetivo evadiendo las rocas altamente filosas, recorriendo el cielo, terreno liso y, si el robot lo considera mejor, por la arena nociva.\n",
        "\n",
        "En términos de Q-Learning, tanto el punto objetivo como las rocas filosas serán zonas terminales."
      ],
      "metadata": {
        "id": "anu9zKbFg0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correr la siguiente celda para la visualización del entorno\n",
        "\n",
        "def environment(planet: [[int]]) -> None:\n",
        "    '''\n",
        "    Draw a planet's area for visualization.\n",
        "\n",
        "    Args:\n",
        "       planet: a NxN list of integers numbers between 0 and 4\n",
        "    Returns:\n",
        "       None\n",
        "    '''\n",
        "    N = len(planet)\n",
        "\n",
        "    data = np.array(planet)\n",
        "\n",
        "    # create discrete colormap\n",
        "    cmap = mpl.colors.ListedColormap(['darkred', 'red', 'indianred', 'green'])\n",
        "    cmap.set_over('plum')\n",
        "    cmap.set_under('plum')\n",
        "\n",
        "    bounds = [0.5, 1.5, 2.5, 3.5, 4.5]\n",
        "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(math.floor(N/3), math.floor(N/3)))\n",
        "\n",
        "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "    width, height = bbox.width, bbox.height\n",
        "\n",
        "    width *= fig.dpi\n",
        "    height *= fig.dpi\n",
        "\n",
        "    plt.title('Ingenuity Tracker')\n",
        "    ax.imshow(data, cmap=cmap, norm=norm)\n",
        "\n",
        "    # draw gridlines\n",
        "    ax.set_xticks(np.arange(-0, N, 1));\n",
        "    ax.set_yticks(np.arange(-0, N, 1));\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def robot_tracker(planet: [[int]], pos: (int,int)) -> None:\n",
        "    '''\n",
        "    Grafica un área del planeta y grafica el robot, si este se encuentra dentro del área.\n",
        "    Diferentes valores de planet[i][j] tendrán diferentes colores en el gráfico.\n",
        "    Función adaptada de:\n",
        "        https://stackoverflow.com/questions/43971138/python-plotting-colored-grid-based-on-values\n",
        "    Requiere refactorización.\n",
        "    Args:\n",
        "        planet:  lista NxN de números enteros entre 0 y 4.\n",
        "        pos: tupla (i,j) de números enteros que representa la posición del robot relativa a la matriz planet.\n",
        "\n",
        "    Retuns:\n",
        "        None\n",
        "    '''\n",
        "    N = len(planet)\n",
        "\n",
        "    data = np.array(planet)\n",
        "\n",
        "    # create discrete colormap\n",
        "    cmap = mpl.colors.ListedColormap(['darkred', 'red', 'indianred', 'green'])\n",
        "    cmap.set_over('plum')\n",
        "    cmap.set_under('plum')\n",
        "\n",
        "    bounds = [0.5, 1.5, 2.5, 3.5, 4.5]\n",
        "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(math.floor(N/3), math.floor(N/3)))\n",
        "\n",
        "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "    width, height = bbox.width, bbox.height\n",
        "\n",
        "    width *= fig.dpi\n",
        "    height *= fig.dpi\n",
        "\n",
        "    plt.title('Ingenuity Tracker')\n",
        "    ax.imshow(data, cmap=cmap, norm=norm)\n",
        "    if (0 <= pos[0] < len(planet[0])) and (0 <= pos[1] < len(planet)):\n",
        "        ax.scatter(pos[1], pos[0], marker='s', s= (width/N)**2, color='black')\n",
        "\n",
        "    # draw gridlines\n",
        "    ax.set_xticks(np.arange(-0, N, 1));\n",
        "    ax.set_yticks(np.arange(-0, N, 1));\n",
        "    clear_output(wait=True)\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "    sleep(1)\n",
        "\n",
        "def robot_show_path(planet: [[int]], positions: [(int, int)]) -> None:\n",
        "    for i in range(len(positions)):\n",
        "        robot_tracker(new_map, positions[i])"
      ],
      "metadata": {
        "id": "4DxeC4xDlFD3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5u-NE9uScteb"
      },
      "outputs": [],
      "source": [
        "# Correr la siguiente celda para definir las funciones auxiliares necesarias\n",
        "# para la creación del mapa.\n",
        "\n",
        "def sand_function(map: [[int]], n: int) -> None:\n",
        "    '''\n",
        "    Changes randomly n elements of the given matrix by number 3.\n",
        "\n",
        "    Args:\n",
        "        map: a NxN list of integers numbers between 0 and 2\n",
        "        n: a positive integer number greater or equal to 0\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "    try:\n",
        "        assert all( all( 0 <= map[i][j] <= 2 for j in range(0,len(map[0])) )  for i in range(0, len(map)) )\n",
        "        assert n >= 0\n",
        "    except:\n",
        "        print('An error has occurred. Check if:\\n \\t*The elements of the given matrix are between 0 and 2.\\n \\t*The given number is a positive integer greater or equal to 0.')\n",
        "        sys.exit()\n",
        "\n",
        "    for i in range(n):\n",
        "        d, e = random.choice([k for k in range(len(map)//2, len(map))]), random.choice([k for k in range(len(map))])\n",
        "        map[d][e] = 3\n",
        "\n",
        "    try:\n",
        "        assert all( all( 0 <= map[i][j] <= 3 for j in range(0,len(map[0])) ) for i in range(0, len(map)) )\n",
        "    except:\n",
        "        print('An error has occurred.')\n",
        "\n",
        "def random_environment(map: [[int]], sand: int) -> [[int]]:\n",
        "    '''\n",
        "    Given a matrix full of zeros and a positive integer, return a matrix that is half filled with random numbers between 0 and 3\n",
        "\n",
        "    Args:\n",
        "      map: a NxN list which elements are zeros\n",
        "      sand: an integer number greater than or equal to 0\n",
        "\n",
        "    Returns:\n",
        "      a matrix that is half filled with random numbers between 0 and 3\n",
        "    '''\n",
        "    try:\n",
        "        assert isinstance(map, list) and isinstance(sand, int)\n",
        "        assert all(all(map[i][j] == 0 for j in range(0,len(map[0]))) for i in range(0,len(map)))\n",
        "        assert sand >= 0\n",
        "    except:\n",
        "        print('An error has ocurred. Check if:\\n \\t*The given matrix is full of zeros.\\n \\t*The given number is greater or equal to zero.\\n \\t*The inputs are a list and an integer respectively.')\n",
        "        sys.exit()\n",
        "\n",
        "    n = len(map)//2\n",
        "    while n != len(map):\n",
        "            m = 0\n",
        "            while m != len(map[0]):\n",
        "                if  n >= (len(map)//2) + 1:\n",
        "                    map[n][m] = 2\n",
        "                    a, b = random.choice([k for k in range(len(map)//2, len(map))]), random.choice([k for k in range(len(map))])\n",
        "                    map[a][b] = 1\n",
        "                else:\n",
        "                    map[n][m] = random.choice([0,1,2])\n",
        "                m += 1\n",
        "            n += 1\n",
        "\n",
        "    for i in range(2, len(map)):\n",
        "        if map[len(map)//2][i-2] == 0:\n",
        "            pass\n",
        "        else:\n",
        "            if map[len(map)//2][i-1] != 0 and map[len(map)//2][i] != 0:\n",
        "                map[len(map)//2 - 1][i-1] = 2\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    if sand == 0:\n",
        "        pass\n",
        "    else:\n",
        "        sand_function(map, sand)\n",
        "\n",
        "    try:\n",
        "        assert all( all( 0 <= map[i][j] <= 3 for j in range(0,len(map[0])) ) for i in range(0, len(map)) )\n",
        "    except:\n",
        "        print('An error has ocurred.')\n",
        "        sys.exit()\n",
        "    return map\n",
        "\n",
        "def final_point(map: [[int]]) -> (int, int):\n",
        "    '''\n",
        "    Ask the user two integers numbers that represent a row and a column of the given matrix,\n",
        "    these numbers represent the objetive of the robot\n",
        "\n",
        "    Args:\n",
        "       map: a NxN list of intengers number between 0 and 4\n",
        "\n",
        "    Returns:\n",
        "       a tuple (i, j) which represents a position relative to the matrix.\n",
        "    '''\n",
        "    try:\n",
        "        assert all( all( 0 <= map[i][j] <= 4 for j in range(0,len(map[0])) ) for i in range(0, len(map)) )\n",
        "    except:\n",
        "        print('An error has occurred. Check in the elements of the given matrix are between 0 and 4.')\n",
        "        sys.exit()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            final_X = int(input('Set your coordenate X for the final point: '))\n",
        "            final_Y = int(input('Set your coordenate Y for the final point: '))\n",
        "            assert 0 <= final_X < len(map) and 0 <= final_Y < len(map[0])\n",
        "            break\n",
        "        except:\n",
        "            print(f'The given numbers must satisfies 0 <= X <= {len(map)-1} and 0 <= Y <= {len(map[0])-1}')\n",
        "\n",
        "    return final_X, final_Y\n",
        "\n",
        "def initial_point(map: [[int]]) -> (int, int):\n",
        "    '''\n",
        "    Ask the user two integers numbers that represent a row and a column of the given matrix,\n",
        "    these numbers represent the initial position of the robot\n",
        "\n",
        "    Args:\n",
        "       map: a NxN list of intengers number between 0 and 4\n",
        "\n",
        "    Returns:\n",
        "       a tuple (i, j) which represents a position relative to the matrix.\n",
        "    '''\n",
        "    try:\n",
        "        assert all( all( 0 <= map[i][j] <= 4 for j in range(0,len(map[0])) ) for i in range(0, len(map)) )\n",
        "    except:\n",
        "        print('An error has occurred. Check in the elements of the given matrix are between 0 and 4.')\n",
        "        sys.exit()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            initial_X = int(input('Set your coordenate X for the initial point: '))\n",
        "            initial_Y = int(input('Set your coordenate Y for the inintal point: '))\n",
        "            assert 0 <= initial_X < len(map) and 0 <= initial_Y < len(map[0])\n",
        "            break\n",
        "        except:\n",
        "            print(f'The given numbers must satisfies 0 <= X <= {len(map)-1} and 0 <= Y <= {len(map[0])-1}')\n",
        "\n",
        "    return initial_X, initial_Y\n",
        "\n",
        "def valid_point(map: [[int]], pos: (int, int)) -> bool:\n",
        "    '''\n",
        "    Indicates if a position (i,j) is a valid position relative to the matrix, i.e.,\n",
        "    if it's in the sky or it is on smooth ground\n",
        "\n",
        "    Args:\n",
        "       map: a NxN list of integers numbers between 0 and 4\n",
        "       pos: a tuple (i,j) of positive integers numbers\n",
        "\n",
        "    Returns:\n",
        "       True if the elemente in the i row an j column is 0 or 2. Else, returns False.\n",
        "    '''\n",
        "    try:\n",
        "        assert isinstance(map, list) and isinstance(pos, tuple)\n",
        "        assert all( all( 0 <= map[i][j] <= 4 for j in range(0,len(map[0])) ) for i in range(0, len(map)) )\n",
        "        assert 0 <= pos[0] < len(map) and 0 <= pos[1] < len(map[0])\n",
        "    except:\n",
        "        print(f'An error has ocurred. Check if:\\n \\t*The matrix´s elements are between 0 and 4.\\n \\t*The tuple´s elements are between 0 and {len(map)-1}.\\n \\t*The inputs are a list and a tuple respectively.')\n",
        "        sys.exit()\n",
        "\n",
        "    is_valid = True\n",
        "    if map[pos[0]][pos[1]] == 1 or map[pos[0]][pos[1]] == 3:\n",
        "        is_valid = not is_valid\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        assert is_valid == (map[pos[0]][pos[1]] == 2) or is_valid == (map[pos[0]][pos[1]] == 0)\n",
        "    except:\n",
        "        print('An error has ocurred')\n",
        "    return is_valid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estados y recompensas\n",
        "\n",
        "Al principio, se inicializa una tabla con todos sus valores en 0. Aquí será en donde se almacenen las actualizaciones de nuestros valores Q, esta tabla tendrá como nombre tabla Q.\n",
        "\n",
        "Por otro lado, se creará una matriz que represente el entorno con las recompensas que recibirá el agente durante la interacción con el mismo. Para este ejemplo, se tiene que las recompensas son:\n",
        "\n",
        "\n",
        "*   Cielo: -1\n",
        "*   Terreno liso: -3\n",
        "*   Arenas nocivas: -5\n",
        "*   Rocas altamente filosas: -100\n",
        "*   Punto objetivo: 100\n",
        "\n",
        "Con estas recompensas se realizará la actualización de los valores Q los cuales servirán como la memoria del agente una vez entrenado en su entorno.\n",
        "\n"
      ],
      "metadata": {
        "id": "a9zCdUqklamC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores Q\n",
        "def q_table(map: [[int]]) -> list:\n",
        "    '''\n",
        "    Given a NxN matrix, generates a 3D array with N rows, N columns and 4 depth dimensions\n",
        "    which elements are zeros.\n",
        "\n",
        "    Args:\n",
        "        map: a NxN list\n",
        "    Returns:\n",
        "        a 3D array with N rows, N columns and 4 depth dimensions which elements are zeros.\n",
        "    '''\n",
        "    try:\n",
        "        assert len(map)>10 and len(map[0])>10\n",
        "    except:\n",
        "        print('An error has occurred. Check if the dimention of the matrix are greater than 10.')\n",
        "        sys.exit()\n",
        "\n",
        "    q_values = np.zeros((len(map), len(map[0]), 4))\n",
        "\n",
        "\n",
        "    return q_values\n",
        "\n",
        "# Recompensas\n",
        "\n",
        "def rewards_function(rewards: [[int]], X: int, Y: int) -> None:\n",
        "    '''\n",
        "    Given a NxN matrix and two positive integers numbers, changes the values in a range\n",
        "    of -100 and -5. For the element rewards[X][Y] changes the value by 100\n",
        "\n",
        "    Args:\n",
        "       rewards: a NxN list of integer numbers between 0 and 4\n",
        "       X: a positive integer\n",
        "       Y: a positive integer\n",
        "\n",
        "    Returns:\n",
        "       None\n",
        "    '''\n",
        "    # PRECONDICIÓN: ( all( all( 0 <= rewards[i][j] <= 4 for j in range(0, len(rewards[0])) ) for i in range(0,len(rewards)) )\n",
        "    #                and 0 <= X < len(rewards) and 0 <= Y < len(rewards[0]) )\n",
        "    # POSTCONDICIÓN: all( all( -100 <= rewards[i][j] <= 100 for j in range(0, len(rewards[0])) ) for i in range(0, len(rewards)) )\n",
        "\n",
        "    try:\n",
        "       assert ( all( all( 0 <= rewards[i][j] <= 4 for j in range(0, len(rewards[0])) ) for i in range(0,len(rewards)) )\n",
        "               and 0 <= X < len(rewards) and 0 <= Y < len(rewards[0]) )\n",
        "    except:\n",
        "        print(f'An error has ocurred. Check if:\\n \\t*The elements of the given matrix are between 0 and 4.\\n \\t*The given numbers must satisfy 0<=X<={len(rewards)-1} and 0<=Y<={len(rewards[0])-1}')\n",
        "        sys.exit()\n",
        "\n",
        "    for k in range(0, len(rewards)):\n",
        "        for n in range(0, len(rewards[0])):\n",
        "            if rewards[k][n] == 0:\n",
        "                rewards[k][n] = -1\n",
        "            elif rewards[k][n] == 1:\n",
        "                rewards[k][n] = -100\n",
        "            elif rewards[k][n] == 2:\n",
        "                rewards[k][n] = -3\n",
        "            else:\n",
        "                rewards[k][n] = -5\n",
        "\n",
        "    rewards[X][Y] = 100\n",
        "\n",
        "    try:\n",
        "        assert all( all( -100 <= rewards[i][j] <= 100 for j in range(0, len(rewards[0])) ) for i in range(0, len(rewards)) )\n",
        "    except:\n",
        "        print('An error has ocurred.')\n",
        "        sys.exit()\n",
        ""
      ],
      "metadata": {
        "id": "wlH8adlolaK1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "Para el entrenamiento del agente se definirán una serie de funciones que nos servirán para el mismo. Para la función train_AI_agent() se toman los valores:\n",
        "\n",
        "*     ϵ = 0.1\n",
        "*     α = 0.9\n",
        "*     γ = 0.9\n",
        "\n",
        "Para que el robot pueda entrenar 100000 episodios. Dichos episodios le serán sufientes al robot para que pueda realizar su recorrido al objetivo. Más adelante en la sección de experimentos se explicará la elección de los valores."
      ],
      "metadata": {
        "id": "JiitvCN6v66q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones auxiliares\n",
        "\n",
        "def terminal_state(rewards: [[int]], new_coord_X: int, new_coord_Y: int) -> bool:\n",
        "    '''\n",
        "    Says if a current position is a terminal state, i.e., if a matrix's element in the given\n",
        "    row and column is -100 or 100.\n",
        "\n",
        "    Args:\n",
        "       rewards: a list of list which elements are between -100 and 100\n",
        "       new_coord_X: an integer that is between 0 and matrix's dimention, last one exclusive\n",
        "       new_coord_Y: an integer that is between 0 and matrix's dimention, last one exclusive\n",
        "\n",
        "    Returns:\n",
        "       True if a matrix's element in the given row and column is -100 or 100. Else, returns False\n",
        "    '''\n",
        "    try:\n",
        "        assert all( all( -100 <= rewards[i][j] <= 100 for j in range(0, len(rewards[0])) ) for i in range(0, len(rewards)) )\n",
        "        assert 0 <= new_coord_X < len(rewards) and 0 <= new_coord_Y < len(rewards[0])\n",
        "    except:\n",
        "        print(f'An error has occurred. Check if:\\n \\t*The elements of the given matrix are between -100 and 100.\\n \\t*The given numbers are between 0 and {len(rewards)-1}')\n",
        "        sys.exit()\n",
        "\n",
        "    if rewards[new_coord_X][new_coord_Y] == -100 or rewards[new_coord_X][new_coord_Y] == 100:\n",
        "        is_terminal_state = True\n",
        "    else:\n",
        "        is_terminal_state = False\n",
        "\n",
        "    try:\n",
        "        assert is_terminal_state == (rewards[new_coord_X][new_coord_Y] == -100) or is_terminal_state == (rewards[new_coord_X][new_coord_Y] == 100)\n",
        "    except:\n",
        "        print('An error has occurred')\n",
        "        sys.exit()\n",
        "\n",
        "    return is_terminal_state\n",
        "\n",
        "def start_point(rewards: list) -> (int, int):\n",
        "    '''\n",
        "    Returns a random start position for the agent can learn\n",
        "\n",
        "    Args:\n",
        "      rewards: a list of list which elements are between -100 and 100\n",
        "\n",
        "    Returns:\n",
        "      A tuple with random numbers which represent a start position.\n",
        "    '''\n",
        "    try:\n",
        "        assert all( all( -100 <= rewards[i][j] <= 100 for j in range(0, len(rewards[0])) ) for i in range(0, len(rewards)) )\n",
        "    except:\n",
        "        print('An error has occurred. Check if the elements of the given matrix are between -100 and 100.')\n",
        "        sys.exit()\n",
        "\n",
        "    new_coord_X, new_coord_Y = random.choice([k for k in range(0,len(rewards))]), random.choice([k for k in range(0,len(rewards[0]))])\n",
        "    while terminal_state(rewards, new_coord_X, new_coord_Y):\n",
        "        new_coord_X, new_coord_Y = random.choice([k for k in range(0,len(rewards))]), random.choice([k for k in range(0,len(rewards[0]))])\n",
        "\n",
        "    try:\n",
        "        assert 0 <= new_coord_X < len(rewards) and 0 <= new_coord_Y < len(rewards[0])\n",
        "    except:\n",
        "        print('An error has occurred.')\n",
        "        sys.exit()\n",
        "\n",
        "    return new_coord_X, new_coord_Y\n",
        "\n",
        "def get_actions(q_values: [[int]], new_coord_X: int, new_coord_Y: int, epsilon: float) -> int:\n",
        "    '''\n",
        "    Given an array, two integer numbers and a float, returns a random number between 0 and\n",
        "    3. This function emulates the ϵ-greedy policy.\n",
        "\n",
        "    Args:\n",
        "       q_values: a 3D array with N rows, N columns and 4 depth dimensions\n",
        "       new_coord_X: a positive integer number\n",
        "       new_coord_Y: a positive integer number\n",
        "       epsilon: a float number between 0 and 1\n",
        "\n",
        "    Returns:\n",
        "       a number that represent the index in which a actions is choosen.\n",
        "    '''\n",
        "    try:\n",
        "        assert 0 <= epsilon <= 1\n",
        "        assert 0 <= new_coord_X < len(q_values) and 0 <= new_coord_Y < len(q_values[0])\n",
        "    except:\n",
        "        print(f'An error has occurred. Check if:\\n \\t*The given integer numbers are between 0 and {len(q_values)-1}\\n \\t*The given float number is between 0 and 1')\n",
        "        sys.exit()\n",
        "\n",
        "    if np.random.random() < epsilon:\n",
        "        index = np.argmax(q_values[new_coord_X, new_coord_Y])\n",
        "    else:\n",
        "        index= random.choice([i for i in range(0,4)])\n",
        "\n",
        "    try:\n",
        "        assert 0<= index < 4\n",
        "    except:\n",
        "        print('An error has occurred.')\n",
        "        sys.exit()\n",
        "    return index\n",
        "\n",
        "def get_locations(new_map: [[int]], new_coord_X: int, new_coord_Y: int, new_action_index: int) -> (int, int):\n",
        "    '''\n",
        "    Returns a tuple (i,j) which represents a position according to the action index given and the\n",
        "    X row and Y column of the matrix\n",
        "\n",
        "    Args:\n",
        "        new_map: a NxN list\n",
        "        new_coord_X: a positive integer number\n",
        "        new_coord_Y: a positive integer number\n",
        "        new_action_index: a positive integer number between 0 and 3\n",
        "    Returns:\n",
        "        a tuple (i,j) which represents a position on the matrix\n",
        "\n",
        "    '''\n",
        "    try:\n",
        "        assert len(new_map) > 0 and len(new_map[0])>0\n",
        "        assert 0 <= new_coord_X < len(new_map) and 0 <= new_coord_Y < len(new_map[0])\n",
        "        assert 0 <= new_action_index <= 3\n",
        "    except:\n",
        "        print(f'An error has occurred. Check if:\\n \\t*The given matrix is not empty.\\n \\t*The given X and Y numbers are between 0 and {len(new_map)-1}.\\n \\t*The given index number is between 0 and 3')\n",
        "        sys.exit()\n",
        "\n",
        "    X, Y = new_coord_X, new_coord_Y\n",
        "\n",
        "    actions = ['up', 'left', 'right', 'down']\n",
        "    if actions[new_action_index] == 'up' and X > 0:\n",
        "        X -= 1\n",
        "    elif actions[new_action_index] == 'left' and Y > 0:\n",
        "        Y -= 1\n",
        "    elif actions[new_action_index] == 'right' and Y < len(new_map[0])-1:\n",
        "        Y += 1\n",
        "    elif actions[new_action_index] == 'down' and X < len(new_map)-1:\n",
        "        X += 1\n",
        "\n",
        "    try:\n",
        "        assert 0 <= X < len(new_map) and 0 <= Y < len(new_map[0])\n",
        "    except:\n",
        "        print('An error has occurred.')\n",
        "        sys.exit()\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def shortest_path(rewards: list, q_values: list, X: int, Y: int) -> [(int, int)]:\n",
        "\n",
        "    short_path = []\n",
        "    if terminal_state(rewards, X, Y):\n",
        "        pass\n",
        "    else:\n",
        "        X_1, Y_1 = X, Y\n",
        "        short_path += [(X, Y)]\n",
        "        while not terminal_state(rewards, X, Y):\n",
        "            new_action_index = get_actions(q_values, X, Y, 1)\n",
        "            X, Y = get_locations(rewards, X, Y, new_action_index)\n",
        "            short_path += [(X, Y)]\n",
        "\n",
        "            if len(short_path) > 2:\n",
        "                if (short_path[len(short_path)-3][0] == short_path[len(short_path)-1][0]) and (short_path[len(short_path)-3][1] == short_path[len(short_path)-1][1]):\n",
        "                    short_path = [(X_1, Y_1)]\n",
        "                    break\n",
        "\n",
        "    return short_path"
      ],
      "metadata": {
        "id": "OPPNCn9FwBJd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenando al Agente de IA usando Q-Learning\n",
        "\n",
        "def train_AI_agent(rewards: list, q_values: list) -> None:\n",
        "    epsilon = 0.1    # valor usado para la política ϵ-greedy\n",
        "    alpha = 0.9      # valor que repesenta el ratio de aprendizaje del agente\n",
        "    gamma = 0.9      # valor que representa el factor de descuento del agente\n",
        "\n",
        "    print('Training...')\n",
        "    for episodes in range(100000):\n",
        "        if episodes % 10000 == 0:\n",
        "            print(f'episodes: {episodes}')\n",
        "\n",
        "        row, column = start_point(rewards)\n",
        "\n",
        "        while not terminal_state(rewards, row, column):\n",
        "            action_index = get_actions(q_values, row, column, epsilon)\n",
        "\n",
        "            old_row, old_column = row, column\n",
        "            row, column = get_locations(rewards, row, column, action_index)\n",
        "\n",
        "            reward = rewards[row][column]\n",
        "            old_q_value = q_values[old_row, old_column, action_index]\n",
        "            TD = reward + (gamma * np.max(q_values[row, column])) - old_q_value\n",
        "\n",
        "            new_q_value = old_q_value + (alpha * TD)\n",
        "            q_values[old_row, old_column, action_index] = new_q_value\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "DkLtl8UWx_BF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resultado Final"
      ],
      "metadata": {
        "id": "dWafgGQWdcYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones para la interaccion con el usuario\n",
        "\n",
        "def change_initial_point() -> bool:\n",
        "    change_i= input('Would you like to change your inital position?\\nYes or No: ')\n",
        "    if change_i == 'Yes' or change_i == 'yes' or change_i == 'YES':\n",
        "        change_i = True\n",
        "    else:\n",
        "        change_i = False\n",
        "    return change_i\n",
        "\n",
        "def change_final_point() -> bool:\n",
        "    change_f = input('Would you like to change your final positions?\\nYes or No: ')\n",
        "    if change_f == 'Yes' or change_f == 'yes' or change_f == 'YES':\n",
        "        change_f = True\n",
        "    else:\n",
        "        change_f = False\n",
        "    return change_f\n",
        "\n",
        "def world_creation() -> (list, int, int):\n",
        "    while True:\n",
        "        try:\n",
        "            N = int(input('Set your map size\\nEnter a number between 11 and 19: '))\n",
        "            assert 11 <= N <= 19\n",
        "            break\n",
        "        except:\n",
        "            print('Invalid number.')\n",
        "\n",
        "    map = []\n",
        "    for n in range(0, N):\n",
        "        map += [[0 for k in range(0, N)]]\n",
        "\n",
        "    sand = input('Do you want your planet to have harmful sand?\\nYes or No: ')\n",
        "    if sand == 'Yes' or sand == 'yes' or sand == 'YES':\n",
        "        sand = int(input('How many harmful sand do you want on your environment?: '))\n",
        "    else:\n",
        "        sand = 0\n",
        "\n",
        "    new_map = random_environment(map, sand)\n",
        "\n",
        "    return new_map, N, sand\n",
        "\n",
        "def wished_world() -> bool:\n",
        "    while True:\n",
        "        try:\n",
        "            is_wished_world = input('This is your generated world. Would you like to change it?\\nYes or No: ')\n",
        "            assert is_wished_world == 'Yes' or is_wished_world == 'yes' or is_wished_world == 'YES' or is_wished_world == 'No' or is_wished_world == 'no' or is_wished_world == 'NO'\n",
        "            break\n",
        "        except:\n",
        "            print('The answer must be a Yes or No answer.\\nIt can be used uppercase or lowercase characters.')\n",
        "\n",
        "    if is_wished_world == 'Yes' or is_wished_world == 'yes' or is_wished_world == 'YES':\n",
        "        is_wished_world = False\n",
        "    else:\n",
        "        is_wished_world = True\n",
        "\n",
        "    return is_wished_world"
      ],
      "metadata": {
        "id": "vYNVLgPOLr-p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_map, N, sand = world_creation()\n",
        "environment(new_map)\n",
        "while not wished_world():\n",
        "    same_values = input('Would you like to use the same old values?\\nYes or No: ')\n",
        "    if same_values == 'Yes' or same_values == 'yes' or same_values == 'YES':\n",
        "        map = []\n",
        "        for n in range(0, N):\n",
        "            map += [[0 for k in range(0, N)]]\n",
        "\n",
        "        new_map = random_environment(map, sand)\n",
        "        environment(new_map)\n",
        "    else:\n",
        "        new_map, N, sand = world_creation()\n",
        "        environment(new_map)\n",
        "\n",
        "objetive = final_point(new_map)\n",
        "while not valid_point(new_map, objetive):\n",
        "    print('The objetive can´t be a sharp rock or a harmful sand. Please enter another objetive.')\n",
        "    objetive = final_point(new_map)\n",
        "\n",
        "new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "environment(new_map)\n",
        "\n",
        "q_values = q_table(new_map)\n",
        "\n",
        "rewards = copy.deepcopy(new_map)\n",
        "rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "train_AI_agent(rewards, q_values)\n",
        "\n",
        "initial_position = initial_point(new_map)\n",
        "while not valid_point(new_map, initial_position):\n",
        "    print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "while len(positions) == 1:\n",
        "    print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "robot_show_path(new_map, positions)\n",
        "print(f'robot´s path: {positions}')\n",
        "\n",
        "review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_initial_point():\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_final_point():\n",
        "    new_map[objetive[0]][objetive[1]] = 2\n",
        "\n",
        "    objetive = final_point(new_map)\n",
        "    while not valid_point(new_map, objetive):\n",
        "        print('The objetive can´t be a sharp rock or harmful sand. Please enter another objetive.')\n",
        "        objetive = final_point(new_map)\n",
        "\n",
        "    new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "    environment(new_map)\n",
        "\n",
        "    q_values = q_table(new_map)\n",
        "\n",
        "    rewards = copy.deepcopy(new_map)\n",
        "    rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "    train_AI_agent(rewards, q_values)\n",
        "\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "    while change_initial_point():\n",
        "        initial_position = initial_point(new_map)\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "        while len(positions) == 1:\n",
        "            print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "            while not valid_point(new_map, initial_position):\n",
        "                print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "                initial_position = initial_point(new_map)\n",
        "\n",
        "            positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "        while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "            robot_show_path(new_map, positions)\n",
        "            print(f'robot´s path: {positions}')\n",
        "            review = input('Would you like to see your robot´s path again?\\nYes or No: ')"
      ],
      "metadata": {
        "id": "utQjF6AzkQgO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "6ea19d07-56aa-4d8f-ffe1-d8681b811d3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "robot´s path: [(10, 7), (11, 7), (11, 6), (11, 5), (12, 5), (13, 5), (14, 5), (14, 4), (14, 3), (14, 2), (14, 1), (14, 0)]\n",
            "Would you like to see your robot´s path again?\n",
            "Yes or No: No\n",
            "Would you like to change your inital position?\n",
            "Yes or No: No\n",
            "Would you like to change your final positions?\n",
            "Yes or No: No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<mark>comentario cchang</mark>\n",
        "\n",
        "Me parece que 100000 episodios puede ser mucho para un mapa de 11x11. Otros proyectos corren con apenas 10000. El entrenamiento debería ser un parámetro que el usuario pueda controlar.\n"
      ],
      "metadata": {
        "id": "MFfKe8C7BUp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentos\n",
        "\n",
        "Se analizará:\n",
        "* Valores distintos de epsilon para ver el comportamiento del agente con respecto a la política ϵ-greedy. Entre estos:\n",
        "  * ϵ = 0.05\n",
        "  * ϵ = 0.5\n",
        "  * ϵ = 0.9\n",
        "* Valores distintos de γ para ver el comportamiento del agente con respecto a la tasa de descuento en su aprendizaje. Entre estos:\n",
        "  * γ = 0.05\n",
        "  * γ = 0.5\n",
        "* Valores distintos de α para ver el comportamiento del agente con respecto al ratio de aprendizaje. Entre estos:\n",
        "  * α = 0.05\n",
        "  * α = 0.5\n",
        "\n",
        "Esto con el fin de ver el comportamiento del agente en su entorno y si es capaz de cumplir con el objetivo."
      ],
      "metadata": {
        "id": "CQDepAmPqwwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primera prueba:\n",
        "### Caso con valores bajos\n",
        "\n",
        "Para este caso donde los valores de ϵ, γ y α son muy bajos el agente es capaz de aprender caminos cortos y sencillos al objetivo, lo que sugiere que es necesario más de 100000 episodios para que logre su objetivo. Sin embargo con mapas más complicados le cuesta más aprender sobre su entorno, pudiendo ser que no llegue a aprender lo suficiente. Además con mapas más grandes de 11 tarda más y mayormente no logra aprender a caminar."
      ],
      "metadata": {
        "id": "4MpBzo94P1A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenando al Agente de IA usando Q-Learning\n",
        "\n",
        "def train_AI_agent_1(rewards: list, q_values: list) -> None:\n",
        "    epsilon = 0.05\n",
        "    alpha = 0.05\n",
        "    gamma = 0.05\n",
        "\n",
        "    print('Training...')\n",
        "    for episodes in range(100000):\n",
        "        if episodes % 1000 == 0:\n",
        "            print(f'episodes: {episodes}')\n",
        "\n",
        "        row, column = start_point(rewards)\n",
        "\n",
        "        while not terminal_state(rewards, row, column):\n",
        "            action_index = get_actions(q_values, row, column, epsilon)\n",
        "\n",
        "            old_row, old_column = row, column\n",
        "            row, column = get_locations(rewards, row, column, action_index)\n",
        "\n",
        "            reward = rewards[row][column]\n",
        "            old_q_value = q_values[old_row, old_column, action_index]\n",
        "            TD = reward + (gamma * np.max(q_values[row, column])) - old_q_value\n",
        "\n",
        "            new_q_value = old_q_value + (alpha * TD)\n",
        "            q_values[old_row, old_column, action_index] = new_q_value\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "lz3-Kes8PKao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_map, N, sand = world_creation()\n",
        "environment(new_map)\n",
        "while not wished_world():\n",
        "    same_values = input('Would you like to use the same old values?\\nYes or No: ')\n",
        "    if same_values == 'Yes' or same_values == 'yes' or same_values == 'YES':\n",
        "        map = []\n",
        "        for n in range(0, N):\n",
        "            map += [[0 for k in range(0, N)]]\n",
        "\n",
        "        new_map = random_environment(map, sand)\n",
        "        environment(new_map)\n",
        "    else:\n",
        "        new_map, N, sand = world_creation()\n",
        "        environment(new_map)\n",
        "\n",
        "objetive = final_point(new_map)\n",
        "while not valid_point(new_map, objetive):\n",
        "    print('The objetive can´t be a sharp rock or a harmful sand. Please enter another objetive.')\n",
        "    objetive = final_point(new_map)\n",
        "\n",
        "new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "environment(new_map)\n",
        "\n",
        "q_values = q_table(new_map)\n",
        "\n",
        "rewards = copy.deepcopy(new_map)\n",
        "rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "train_AI_agent_1(rewards, q_values)\n",
        "\n",
        "initial_position = initial_point(new_map)\n",
        "while not valid_point(new_map, initial_position):\n",
        "    print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "while len(positions) == 1:\n",
        "    print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "robot_show_path(new_map, positions)\n",
        "print(f'robot´s path: {positions}')\n",
        "\n",
        "review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_initial_point():\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_final_point():\n",
        "    new_map[objetive[0]][objetive[1]] = 2\n",
        "\n",
        "    objetive = final_point(new_map)\n",
        "    while not valid_point(new_map, objetive):\n",
        "        print('The objetive can´t be a sharp rock or harmful sand. Please enter another objetive.')\n",
        "        objetive = final_point(new_map)\n",
        "\n",
        "    new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "    environment(new_map)\n",
        "\n",
        "    q_values = q_table(new_map)\n",
        "\n",
        "    rewards = copy.deepcopy(new_map)\n",
        "    rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "    train_AI_agent_1(rewards, q_values)\n",
        "\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "    while change_initial_point():\n",
        "        initial_position = initial_point(new_map)\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "        while len(positions) == 1:\n",
        "            print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "            while not valid_point(new_map, initial_position):\n",
        "                print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "                initial_position = initial_point(new_map)\n",
        "\n",
        "            positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "        while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "            robot_show_path(new_map, positions)\n",
        "            print(f'robot´s path: {positions}')\n",
        "            review = input('Would you like to see your robot´s path again?\\nYes or No: ')"
      ],
      "metadata": {
        "id": "o4-r-YU82rPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segunda Prueba:\n",
        "### Caso donde los valores son intermedios\n",
        "\n",
        "Para este caso donde ϵ, γ y α son valores intermedios se puede notar que el agente, para mapas de tamaño 11x11, tarda menos en aprender a llegar al punto objetivo. Sin embargo, con mapas de mayor tamaño tarda mucho más o no consigue aprender. Adicionalmente, la complejitud de los mapas influyen en su aprendizaje, consiguiendo mejores desempeño en un mapa sin muchos obstáculos."
      ],
      "metadata": {
        "id": "CaCrxqXHQDaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenando al Agente de IA usando Q-Learning\n",
        "\n",
        "def train_AI_agent_2(rewards: list, q_values: list) -> None:\n",
        "    epsilon = 0.5\n",
        "    alpha = 0.5\n",
        "    gamma = 0.5\n",
        "\n",
        "    print('Training...')\n",
        "    for episodes in range(100000):\n",
        "        if episodes % 1000 == 0:\n",
        "            print(f'episodes: {episodes}')\n",
        "\n",
        "        row, column = start_point(rewards)\n",
        "\n",
        "        while not terminal_state(rewards, row, column):\n",
        "            action_index = get_actions(q_values, row, column, epsilon)\n",
        "\n",
        "            old_row, old_column = row, column\n",
        "            row, column = get_locations(rewards, row, column, action_index)\n",
        "\n",
        "            reward = rewards[row][column]\n",
        "            old_q_value = q_values[old_row, old_column, action_index]\n",
        "            TD = reward + (gamma * np.max(q_values[row, column])) - old_q_value\n",
        "\n",
        "            new_q_value = old_q_value + (alpha * TD)\n",
        "            q_values[old_row, old_column, action_index] = new_q_value\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "QQTAi0i8PiP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_map, N, sand = world_creation()\n",
        "environment(new_map)\n",
        "while not wished_world():\n",
        "    same_values = input('Would you like to use the same old values?\\nYes or No: ')\n",
        "    if same_values == 'Yes' or same_values == 'yes' or same_values == 'YES':\n",
        "        map = []\n",
        "        for n in range(0, N):\n",
        "            map += [[0 for k in range(0, N)]]\n",
        "\n",
        "        new_map = random_environment(map, sand)\n",
        "        environment(new_map)\n",
        "    else:\n",
        "        new_map, N, sand = world_creation()\n",
        "        environment(new_map)\n",
        "\n",
        "objetive = final_point(new_map)\n",
        "while not valid_point(new_map, objetive):\n",
        "    print('The objetive can´t be a sharp rock or a harmful sand. Please enter another objetive.')\n",
        "    objetive = final_point(new_map)\n",
        "\n",
        "new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "environment(new_map)\n",
        "\n",
        "q_values = q_table(new_map)\n",
        "\n",
        "rewards = copy.deepcopy(new_map)\n",
        "rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "train_AI_agent_2(rewards, q_values)\n",
        "\n",
        "initial_position = initial_point(new_map)\n",
        "while not valid_point(new_map, initial_position):\n",
        "    print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "while len(positions) == 1:\n",
        "    print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "robot_show_path(new_map, positions)\n",
        "print(f'robot´s path: {positions}')\n",
        "\n",
        "review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_initial_point():\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_final_point():\n",
        "    new_map[objetive[0]][objetive[1]] = 2\n",
        "\n",
        "    objetive = final_point(new_map)\n",
        "    while not valid_point(new_map, objetive):\n",
        "        print('The objetive can´t be a sharp rock or harmful sand. Please enter another objetive.')\n",
        "        objetive = final_point(new_map)\n",
        "\n",
        "    new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "    environment(new_map)\n",
        "\n",
        "    q_values = q_table(new_map)\n",
        "\n",
        "    rewards = copy.deepcopy(new_map)\n",
        "    rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "    train_AI_agent_2(rewards, q_values)\n",
        "\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "    while change_initial_point():\n",
        "        initial_position = initial_point(new_map)\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "        while len(positions) == 1:\n",
        "            print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "            while not valid_point(new_map, initial_position):\n",
        "                print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "                initial_position = initial_point(new_map)\n",
        "\n",
        "            positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "        while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "            robot_show_path(new_map, positions)\n",
        "            print(f'robot´s path: {positions}')\n",
        "            review = input('Would you like to see your robot´s path again?\\nYes or No: ')"
      ],
      "metadata": {
        "id": "4BdWeDuo7vVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tercera Prueba:\n",
        "### Caso donde los valores son altos\n",
        "\n",
        "En el caso donde los valores de ϵ, γ y α son muy altos, el agente es capaz de aprender a recorrer el mapa para objetivos ubicados en el cielo, pues con otros objetivos un poco complejos que presenten más obstáculos no logra aprender a llegar al mismo."
      ],
      "metadata": {
        "id": "NfPf55C8ixG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenando al Agente de IA usando Q-Learning\n",
        "\n",
        "def train_AI_agent_3(rewards: list, q_values: list) -> None:\n",
        "    epsilon = 0.9\n",
        "    alpha = 0.9\n",
        "    gamma = 0.9\n",
        "\n",
        "    print('Training...')\n",
        "    for episodes in range(100000):\n",
        "        if episodes % 1000 == 0:\n",
        "            print(f'episodes: {episodes}')\n",
        "\n",
        "        row, column = start_point(rewards)\n",
        "\n",
        "        while not terminal_state(rewards, row, column):\n",
        "            action_index = get_actions(q_values, row, column, epsilon)\n",
        "\n",
        "            old_row, old_column = row, column\n",
        "            row, column = get_locations(rewards, row, column, action_index)\n",
        "\n",
        "            reward = rewards[row][column]\n",
        "            old_q_value = q_values[old_row, old_column, action_index]\n",
        "            TD = reward + (gamma * np.max(q_values[row, column])) - old_q_value\n",
        "\n",
        "            new_q_value = old_q_value + (alpha * TD)\n",
        "            q_values[old_row, old_column, action_index] = new_q_value\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "GQhIWZ3DPj-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_map, N, sand = world_creation()\n",
        "environment(new_map)\n",
        "while not wished_world():\n",
        "    same_values = input('Would you like to use the same old values?\\nYes or No: ')\n",
        "    if same_values == 'Yes' or same_values == 'yes' or same_values == 'YES':\n",
        "        map = []\n",
        "        for n in range(0, N):\n",
        "            map += [[0 for k in range(0, N)]]\n",
        "\n",
        "        new_map = random_environment(map, sand)\n",
        "        environment(new_map)\n",
        "    else:\n",
        "        new_map, N, sand = world_creation()\n",
        "        environment(new_map)\n",
        "\n",
        "objetive = final_point(new_map)\n",
        "while not valid_point(new_map, objetive):\n",
        "    print('The objetive can´t be a sharp rock or a harmful sand. Please enter another objetive.')\n",
        "    objetive = final_point(new_map)\n",
        "\n",
        "new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "environment(new_map)\n",
        "\n",
        "q_values = q_table(new_map)\n",
        "\n",
        "rewards = copy.deepcopy(new_map)\n",
        "rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "train_AI_agent_3(rewards, q_values)\n",
        "\n",
        "initial_position = initial_point(new_map)\n",
        "while not valid_point(new_map, initial_position):\n",
        "    print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "while len(positions) == 1:\n",
        "    print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "robot_show_path(new_map, positions)\n",
        "print(f'robot´s path: {positions}')\n",
        "\n",
        "review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_initial_point():\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_final_point():\n",
        "    new_map[objetive[0]][objetive[1]] = 2\n",
        "\n",
        "    objetive = final_point(new_map)\n",
        "    while not valid_point(new_map, objetive):\n",
        "        print('The objetive can´t be a sharp rock or harmful sand. Please enter another objetive.')\n",
        "        objetive = final_point(new_map)\n",
        "\n",
        "    new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "    environment(new_map)\n",
        "\n",
        "    q_values = q_table(new_map)\n",
        "\n",
        "    rewards = copy.deepcopy(new_map)\n",
        "    rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "    train_AI_agent_3(rewards, q_values)\n",
        "\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "    while change_initial_point():\n",
        "        initial_position = initial_point(new_map)\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "        while len(positions) == 1:\n",
        "            print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "            while not valid_point(new_map, initial_position):\n",
        "                print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "                initial_position = initial_point(new_map)\n",
        "\n",
        "            positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "        while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "            robot_show_path(new_map, positions)\n",
        "            print(f'robot´s path: {positions}')\n",
        "            review = input('Would you like to see your robot´s path again?\\nYes or No: ')"
      ],
      "metadata": {
        "id": "6pp_bVvJ9NGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prueba Cuatro:\n",
        "### Valores de γ y α bajos con valor de ϵ alto\n",
        "\n",
        "Con valores de γ y α bajos pero con un valor de ϵ alto, el agente puede aprender a llegar a objetivos ubicado en el cielo mediante caminos sencillos, de resto no logra aprender con su entorno."
      ],
      "metadata": {
        "id": "jXtcQ8L2iyAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenando al Agente de IA usando Q-Learning\n",
        "\n",
        "def train_AI_agent_4(rewards: list, q_values: list) -> None:\n",
        "    epsilon = 0.9\n",
        "    alpha = 0.05\n",
        "    gamma = 0.05\n",
        "\n",
        "    print('Training...')\n",
        "    for episodes in range(100000):\n",
        "        if episodes % 1000 == 0:\n",
        "            print(f'episodes: {episodes}')\n",
        "\n",
        "        row, column = start_point(rewards)\n",
        "\n",
        "        while not terminal_state(rewards, row, column):\n",
        "            action_index = get_actions(q_values, row, column, epsilon)\n",
        "\n",
        "            old_row, old_column = row, column\n",
        "            row, column = get_locations(rewards, row, column, action_index)\n",
        "\n",
        "            reward = rewards[row][column]\n",
        "            old_q_value = q_values[old_row, old_column, action_index]\n",
        "            TD = reward + (gamma * np.max(q_values[row, column])) - old_q_value\n",
        "\n",
        "            new_q_value = old_q_value + (alpha * TD)\n",
        "            q_values[old_row, old_column, action_index] = new_q_value\n",
        "    print('Training completed')"
      ],
      "metadata": {
        "id": "S49VpgvnPkpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_map, N, sand = world_creation()\n",
        "environment(new_map)\n",
        "while not wished_world():\n",
        "    same_values = input('Would you like to use the same old values?\\nYes or No: ')\n",
        "    if same_values == 'Yes' or same_values == 'yes' or same_values == 'YES':\n",
        "        map = []\n",
        "        for n in range(0, N):\n",
        "            map += [[0 for k in range(0, N)]]\n",
        "\n",
        "        new_map = random_environment(map, sand)\n",
        "        environment(new_map)\n",
        "    else:\n",
        "        new_map, N, sand = world_creation()\n",
        "        environment(new_map)\n",
        "\n",
        "objetive = final_point(new_map)\n",
        "while not valid_point(new_map, objetive):\n",
        "    print('The objetive can´t be a sharp rock or a harmful sand. Please enter another objetive.')\n",
        "    objetive = final_point(new_map)\n",
        "\n",
        "new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "environment(new_map)\n",
        "\n",
        "q_values = q_table(new_map)\n",
        "\n",
        "rewards = copy.deepcopy(new_map)\n",
        "rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "train_AI_agent_4(rewards, q_values)\n",
        "\n",
        "initial_position = initial_point(new_map)\n",
        "while not valid_point(new_map, initial_position):\n",
        "    print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "while len(positions) == 1:\n",
        "    print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "robot_show_path(new_map, positions)\n",
        "print(f'robot´s path: {positions}')\n",
        "\n",
        "review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_initial_point():\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "while change_final_point():\n",
        "    new_map[objetive[0]][objetive[1]] = 2\n",
        "\n",
        "    objetive = final_point(new_map)\n",
        "    while not valid_point(new_map, objetive):\n",
        "        print('The objetive can´t be a sharp rock or harmful sand. Please enter another objetive.')\n",
        "        objetive = final_point(new_map)\n",
        "\n",
        "    new_map[objetive[0]][objetive[1]] = 4\n",
        "\n",
        "    environment(new_map)\n",
        "\n",
        "    q_values = q_table(new_map)\n",
        "\n",
        "    rewards = copy.deepcopy(new_map)\n",
        "    rewards_function(rewards, objetive[0], objetive[1])\n",
        "\n",
        "    train_AI_agent_4(rewards, q_values)\n",
        "\n",
        "    initial_position = initial_point(new_map)\n",
        "\n",
        "    while not valid_point(new_map, initial_position):\n",
        "        print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "    positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "    while len(positions) == 1:\n",
        "        print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "        initial_position = initial_point(new_map)\n",
        "\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "    robot_show_path(new_map, positions)\n",
        "    print(f'robot´s path: {positions}')\n",
        "\n",
        "    review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "    while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "\n",
        "    while change_initial_point():\n",
        "        initial_position = initial_point(new_map)\n",
        "        while not valid_point(new_map, initial_position):\n",
        "            print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "        positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "        while len(positions) == 1:\n",
        "            print('The robot can´t go to it´s objetive because is trapped.\\nPlease enter another initial position.')\n",
        "            initial_position = initial_point(new_map)\n",
        "\n",
        "            while not valid_point(new_map, initial_position):\n",
        "                print('The initial position can´t be a sharp rock or harmful sand. Please enter another position.')\n",
        "                initial_position = initial_point(new_map)\n",
        "\n",
        "            positions = shortest_path(rewards, q_values, initial_position[0], initial_position[1])\n",
        "\n",
        "        robot_show_path(new_map, positions)\n",
        "        print(f'robot´s path: {positions}')\n",
        "\n",
        "        review = input('Would you like to see your robot´s path again?\\nYes or No: ')\n",
        "        while review == 'Yes' or review == 'yes' or review == 'YES':\n",
        "            robot_show_path(new_map, positions)\n",
        "            print(f'robot´s path: {positions}')\n",
        "            review = input('Would you like to see your robot´s path again?\\nYes or No: ')"
      ],
      "metadata": {
        "id": "BqSzo7HqBsO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "\n",
        "Una vez analizados los casos con los diferentes valores de ϵ, γ y α se puede ver que las condiciones ideales para que el robot aprenda a realizar su recorrido es con valores de α y γ altos.\n",
        "\n",
        "Para determinar los valores de ϵ se puede notar que con valores muy altos de éste el robot es incapaz de aprender el recorrido debido a la complejitud y aleatoriedad del mapa. Por tal motivo, valores por debajo de 0.5 ayudan a que el agente sea capaz de aprender con mayor precisión el recorrido. Cabe destacar que de los experimentos realizados se puede apreciar que mientras el valor de ϵ sea más menor, mayores episodios necesitará para cumplir con el aprendizaje.\n",
        "\n",
        "Por lo tanto, para nuestro caso donde el agente interactúa con un entorno con diversos obstáculos y caminos complicados un valor bajo de ϵ con valores altos de γ y α se obtiene el desempeño esperado realizando 100000 episodios."
      ],
      "metadata": {
        "id": "FkYjoUehx476"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agradecimientos\n",
        "\n",
        "A Jesús Cuéllar por ayudarme con el entendimiento del aprendizaje del robot. Y agradecimientos especiales a Junior Lara por ayudarme a resolver fallas con el entrenamiento y la ruta del robot en casos donde éste se quedaba sin formas de llegar a su punto objetivo, además de ayudarme con el entendimiento del aprendizaje del agente.\n"
      ],
      "metadata": {
        "id": "OD6g7RxXt1wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n",
        "\n",
        "https://es.acervolima.com/q-learning-en-python/"
      ],
      "metadata": {
        "id": "FYjjwZ9eLBpF"
      }
    }
  ]
}